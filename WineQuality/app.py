"""
ML Assignment 2 - Streamlit Web Application
Wine Quality Classifiction Model Comparison
"""

import streamlit as st
import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.metrics import (
    accuracy_score,
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score,
    matthews_corrcoef,
    confusion_matrix,
    classification_report
)
import seaborn as sns
import matplotlib.pyplot as plt

# Page configuration
st.set_page_config(
    page_title="Wine Quality Classification",
    page_icon="üç∑",
    layout="wide"
)

BASE_DIR = Path(__file__).resolve().parent
MODEL_DIR = BASE_DIR / "model"

# Title and description
st.title("üç∑ Wine Quality Classification App")
st.markdown("""
This application demonstrates **6 Machine Learning Classification Models** trained on the Wine Quality dataset.
Upload test data and select a model to see predictions and evaluation metrics.
""")

st.divider()

# Sidebar for model selection
st.sidebar.header("Model Configuration")

# Model selection dropdown
model_options = {
    'Logistic Regression': str(MODEL_DIR / 'logistic_regression_model.pkl'),
    'Decision Tree': str(MODEL_DIR / 'decision_tree_model.pkl'),
    'K-Nearest Neighbors': str(MODEL_DIR / 'knn_model.pkl'),
    'Naive Bayes': str(MODEL_DIR / 'naive_bayes_model.pkl'),
    'Random Forest': str(MODEL_DIR / 'random_forest_model.pkl'),
    'XGBoost': str(MODEL_DIR / 'xgboost_model.pkl')
}

selected_model_name = st.sidebar.selectbox(
    "Select Classification Model",
    options=list(model_options.keys()),
    help="Choose a model to make predictions and view metrics"
)

st.sidebar.divider()
st.sidebar.info("""
**Dataset Features:**
- Fixed Acidity
- Volatile Acidity
- Citric Acid
- Residual Sugar
- Chlorides
- Free Sulfur Dioxide
- Total Sulfur Dioxide
- Density
- pH
- Sulphates
- Alcohol

**Target:** Wine Quality (Binary)
- 0: Bad Wine (quality < 6)
- 1: Good Wine (quality >= 6)
""")

# File upload section
st.header("üìÇ Upload Test Data")
uploaded_file = st.file_uploader(
    "Upload CSV file with test data",
    type=['csv'],
    help="Upload a CSV file containing wine features and quality_class column"
)

if uploaded_file is not None:
    try:
        # Load the uploaded data
        test_data = pd.read_csv(uploaded_file)
        
        st.success(f"‚úÖ File uploaded successfully! Shape: {test_data.shape}")
        
        # Show data preview
        with st.expander("üìä View Data Preview"):
            st.dataframe(test_data.head(10), use_container_width=True)
            st.write(f"**Total samples:** {len(test_data)}")
            st.write(f"**Features:** {test_data.shape[1] - 1}")
        
        # Check if quality_class column exists
        if 'quality_class' not in test_data.columns:
            st.error("‚ùå The uploaded file must contain a 'quality_class' column!")
            st.stop()
        
        # Separate features and target
        X_test = test_data.drop('quality_class', axis=1)
        y_test = test_data['quality_class']
        
        # Load scaler
        try:
            scaler = pickle.load(open(MODEL_DIR / 'scaler.pkl', 'rb'))
            X_test_scaled = scaler.transform(X_test)
        except FileNotFoundError:
            st.warning("‚ö†Ô∏è Scaler not found. Using unscaled features.")
            X_test_scaled = X_test.values
        
        # Load selected model
        try:
            model = pickle.load(open(model_options[selected_model_name], 'rb'))
            st.success(f"‚úÖ Loaded model: **{selected_model_name}**")
        except FileNotFoundError:
            st.error(f"‚ùå Model file not found: {model_options[selected_model_name]}")
            st.stop()
        
        # Make predictions
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        
        st.divider()
        
        # Display evaluation metrics
        st.header("üìà Evaluation Metrics")
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_pred_proba)
        precision = precision_score(y_test, y_pred, average='binary')
        recall = recall_score(y_test, y_pred, average='binary')
        f1 = f1_score(y_test, y_pred, average='binary')
        mcc = matthews_corrcoef(y_test, y_pred)
        
        # Display metrics in columns
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Accuracy", f"{accuracy:.4f}")
            st.metric("AUC Score", f"{auc:.4f}")
        
        with col2:
            st.metric("Precision", f"{precision:.4f}")
            st.metric("Recall", f"{recall:.4f}")
        
        with col3:
            st.metric("F1 Score", f"{f1:.4f}")
            st.metric("MCC Score", f"{mcc:.4f}")
        
        st.divider()
        
        # Confusion Matrix and Classification Report
        st.header("üéØ Model Performance Analysis")
        
        col_left, col_right = st.columns(2)
        
        with col_left:
            st.subheader("Confusion Matrix")
            cm = confusion_matrix(y_test, y_pred)
            
            fig, ax = plt.subplots(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['Bad Wine (0)', 'Good Wine (1)'],
                       yticklabels=['Bad Wine (0)', 'Good Wine (1)'],
                       ax=ax)
            ax.set_xlabel('Predicted Label', fontsize=12)
            ax.set_ylabel('True Label', fontsize=12)
            ax.set_title(f'Confusion Matrix - {selected_model_name}', fontsize=14)
            st.pyplot(fig)
            plt.close()
        
        with col_right:
            st.subheader("Classification Report")
            report = classification_report(y_test, y_pred, 
                                          target_names=['Bad Wine (0)', 'Good Wine (1)'],
                                          output_dict=True)
            report_df = pd.DataFrame(report).transpose()
            st.dataframe(report_df.style.format("{:.4f}"), use_container_width=True)
        
        st.divider()
        
        # Model comparison (if results file exists)
        st.header("üèÜ Model Comparison")
        try:
            results_df = pd.read_csv(MODEL_DIR / 'model_results.csv')
            st.dataframe(results_df.style.format({
                'Accuracy': '{:.4f}',
                'AUC': '{:.4f}',
                'Precision': '{:.4f}',
                'Recall': '{:.4f}',
                'F1': '{:.4f}',
                'MCC': '{:.4f}'
            }), use_container_width=True)
            
            # Highlight best model for each metric
            st.markdown("**üåü Best Models by Metric:**")
            metrics = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']
            best_models = {}
            for metric in metrics:
                best_idx = results_df[metric].idxmax()
                best_models[metric] = results_df.loc[best_idx, 'Model']
            
            cols = st.columns(3)
            for idx, (metric, model) in enumerate(best_models.items()):
                with cols[idx % 3]:
                    st.info(f"**{metric}:** {model}")
            
        except FileNotFoundError:
            st.warning("‚ö†Ô∏è Model comparison results not found. Train models first using train_models.py")
        
        st.divider()
        
        # Predictions preview
        with st.expander("üîç View Predictions (First 20 samples)"):
            predictions_df = pd.DataFrame({
                'True Label': y_test.values[:20],
                'Predicted Label': y_pred[:20],
                'Prediction Probability': y_pred_proba[:20],
                'Correct': (y_test.values[:20] == y_pred[:20])
            })
            st.dataframe(predictions_df, use_container_width=True)
            
            correct_count = (y_test == y_pred).sum()
            st.write(f"**Total Correct Predictions:** {correct_count} / {len(y_test)} ({100*correct_count/len(y_test):.2f}%)")
        
    except Exception as e:
        st.error(f"‚ùå An error occurred: {str(e)}")
        st.exception(e)

else:
    st.info("üëÜ Please upload a CSV file to begin evaluation")
    st.markdown("""
    **Expected CSV Format:**
    - Features: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, 
      free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol
    - Target: quality_class (0 or 1)
    
    **Note:** The test data file is available in the `model/` directory after training.
    """)

# Footer
st.divider()
st.markdown("""
**ML Assignment 2** | M.Tech AIML | Wine Quality Classification | January 2026
""")


